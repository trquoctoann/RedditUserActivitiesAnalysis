{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07100e8-30a4-41c6-8959-e66cb2b0ba40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import flask\n",
    "import pandas as pd\n",
    "from engine import TextClassificationEngine, TopicModellingModel\n",
    "from utilities import send_request_reddit_get_new_post\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30079b0-aafe-465a-81e4-5788e0dfd595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "           .appName('Web Appp') \\\n",
    "           .config(\"spark.jars\", \"mysql-connector-j-8.0.32.jar\")\\\n",
    "           .config(\"spark.driver.memory\", \"6g\") \\\n",
    "           .config(\"spark.executor.memory\", \"8g\") \\\n",
    "           .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29065808-fb80-4462-89d1-694dbec82e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7137650a-2a4b-4cea-9e11-be4d922d3c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:engine:Starting up text classification engine: \n",
      "INFO:engine:Loading labled data...\n",
      "INFO:engine:Loading completed\n",
      "INFO:engine:Preprocessing data...\n",
      "INFO:engine:Preprocessing completed\n",
      "INFO:engine:Vectorize data...\n",
      "INFO:engine:Vectorization completed\n",
      "INFO:engine:Training text classification model...\n",
      "INFO:engine:Text classification model built!\n"
     ]
    }
   ],
   "source": [
    "label_engine = TextClassificationEngine(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ecd1de2-a641-4c2e-8159-2be239958db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "INFO:engine:Starting up model LDA Business: \n",
      "INFO:engine:Loading data...\n",
      "INFO:engine:Loading completed\n",
      "INFO:engine:Preprocessing data...\n",
      "INFO:engine:Preprocessing completed\n",
      "INFO:engine:LDA Business model built!\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/work/web_app/engine.py\", line 159, in <lambda>\n    max_index = udf(lambda x: x.index(max(x)) if x is not None else None, IntegerType())\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 195, in max\n    return _invoke_function_over_columns(\"max\", col)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 93, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 93, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: [0.02001713277407433, 0.017418567792321944, 0.4354759344044246, 0.5043231937843052, 0.022765171244873837] of type <class 'list'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m predicted_topic \u001b[38;5;241m=\u001b[39m topic_engine\u001b[38;5;241m.\u001b[39mpredict_topic(considered_post)\n\u001b[1;32m     22\u001b[0m grouped_topic \u001b[38;5;241m=\u001b[39m predicted_topic\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m---> 23\u001b[0m max_count \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped_topic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m highest_count_groups \u001b[38;5;241m=\u001b[39m grouped_topic\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m max_count)\n\u001b[1;32m     26\u001b[0m topic \u001b[38;5;241m=\u001b[39m highest_count_groups\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfirst()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1938\u001b[0m, in \u001b[0;36mDataFrame.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Row]:\n\u001b[1;32m   1929\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first row as a :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1930\u001b[0m \n\u001b[1;32m   1931\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;124;03m    Row(age=2, name='Alice')\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1924\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \n\u001b[1;32m   1899\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1924\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(n)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1926\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:868\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m    859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/work/web_app/engine.py\", line 159, in <lambda>\n    max_index = udf(lambda x: x.index(max(x)) if x is not None else None, IntegerType())\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 195, in max\n    return _invoke_function_over_columns(\"max\", col)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 93, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\", line 93, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 65, in _to_java_column\n    raise TypeError(\nTypeError: Invalid argument, not a string or column: [0.02001713277407433, 0.017418567792321944, 0.4354759344044246, 0.5043231937843052, 0.022765171244873837] of type <class 'list'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n"
     ]
    }
   ],
   "source": [
    "username = 'AutomaticPepper1040'\n",
    "url = f'https://oauth.reddit.com/user/{username}/submitted'\n",
    "response, status = send_request_reddit_get_new_post(url)\n",
    "total_post = pd.DataFrame(columns = ['post_id', 'descriptions'])\n",
    "for post in response['data']['children']:\n",
    "    post_id = post['data']['id']\n",
    "    descriptions = post['data']['title']\n",
    "    total_post_aux = pd.DataFrame({'post_id': [post_id], 'descriptions': [descriptions]})\n",
    "    total_post = pd.concat([total_post_aux, total_post], ignore_index = True, axis = 0)\n",
    "\n",
    "predicted_label = label_engine.predict_label(total_post)\n",
    "grouped_label = predicted_label.groupBy(\"label_name\").count()\n",
    "max_count = grouped_label.agg(max(\"count\")).first()[0]\n",
    "\n",
    "highest_count_groups = grouped_label.filter(col('count') == max_count)\n",
    "label_name = highest_count_groups.select('label_name').first()[0]\n",
    "considered_post = predicted_label.filter(col('label_name') == label_name)\n",
    "topic_engine = TopicModellingModel(spark, label_name)\n",
    "\n",
    "predicted_topic = topic_engine.predict_topic(considered_post)\n",
    "\n",
    "grouped_topic = predicted_topic.groupBy(\"topic\").count()\n",
    "max_count = grouped_topic.agg(max(\"count\")).first()[0]\n",
    "\n",
    "highest_count_groups = grouped_topic.filter(col('count') == max_count)\n",
    "topic = highest_count_groups.select('topic').first()[0]\n",
    "results = topic_engine.get_recommendation(topic)\n",
    "recommendations = []\n",
    "for result in results: \n",
    "    recommendation = []\n",
    "    descriptions = result[2]\n",
    "    created_utc = result[3]\n",
    "    source_url = result[4]\n",
    "    post_url = result[5]\n",
    "    category = result[6]\n",
    "    recommendation.append(descriptions)\n",
    "    recommendation.append(created_utc)\n",
    "    recommendation.append(source_url)\n",
    "    recommendation.append(post_url)\n",
    "    recommendation.append(category)\n",
    "    recommendations.append(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab00cd4-d36a-4bd3-a0f8-f159b491293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = flask.Flask(__name__, template_folder = os.path.join(path, 'template'))\n",
    "# @app.route('/', methods = ['GET', 'POST'])\n",
    "# def main():\n",
    "#     if flask.request.method == 'GET':\n",
    "#         return flask.render_template('index.html')\n",
    "            \n",
    "#     if flask.request.method == 'POST':\n",
    "#         username = flask.request.form['user_id']\n",
    "#         url = f'https://oauth.reddit.com/user/{username}/submitted'\n",
    "#         response, status = send_request_reddit_get_new_post(url)\n",
    "#         total_post = pd.DataFrame(columns = ['post_id', 'descriptions'])\n",
    "#         for post in response['data']['children']:\n",
    "#             post_id = post['data']['id']\n",
    "#             descriptions = post['data']['title']\n",
    "#             total_post_aux = pd.DataFrame({'post_id': [post_id], 'descriptions': [descriptions]})\n",
    "#             total_post = pd.concat([total_post_aux, total_post], ignore_index = True, axis = 0)\n",
    "            \n",
    "#         predicted_label = label_engine.predict_label(total_post)\n",
    "#         grouped_label = predicted_label.groupBy(\"label_name\").count()\n",
    "#         max_count = grouped_label.agg(max(\"count\")).first()[0]\n",
    "\n",
    "#         highest_count_groups = grouped_label.filter(col('count') == max_count)\n",
    "#         label_name = highest_count_groups.select('label_name').first()[0]\n",
    "#         considered_post = predicted_label.filter(col('label_name') == label_name)\n",
    "#         topic_engine = TopicModellingModel(spark, label_name)\n",
    "        \n",
    "#         predicted_topic = topic_engine.predict_topic(considered_post)\n",
    "        \n",
    "#         grouped_topic = predicted_topic.groupBy(\"topic\").count()\n",
    "#         max_count = grouped_topic.agg(max(\"count\")).first()[0]\n",
    "\n",
    "#         highest_count_groups = grouped_label.filter(col('count') == max_count)\n",
    "#         topic = highest_count_groups.select('topic').first()[0]\n",
    "#         results = topic_engine.get_recommendation(topic)\n",
    "#         recommendations = []\n",
    "#         for result in results: \n",
    "#             recommendation = []\n",
    "#             descriptions = result[2]\n",
    "#             created_utc = result[3]\n",
    "#             source_url = result[4]\n",
    "#             post_url = result[5]\n",
    "#             category = result[6]\n",
    "#             recommendation.append(descriptions)\n",
    "#             recommendation.append(created_utc)\n",
    "#             recommendation.append(source_url)\n",
    "#             recommendation.append(post_url)\n",
    "#             recommendation.append(category)\n",
    "#             recommendations.append(recommendation)\n",
    "#         return flask.render_template('result.html', recommendations = recommendations)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03aa77-76de-43c9-b14f-d20b71098906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
